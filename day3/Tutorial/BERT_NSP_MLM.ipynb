{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# BERT 基本動作\n",
    "BERT (Bidirectional Encoder Representations from Transofmers) は NLP の事前学習 (Pre-Trained) モデルです。文の単語それぞれの Embedding (埋め込み表現) を出力します。また、word2vec などの既存手法と異なり、文脈を考慮するのと、いわゆる一般的な深層学習モデルと同様にモデルの目的に応じて柔軟にネットワークが変更できるというメリットがあります。\n",
    "\n",
    "本ノートブックでは、事前学習で用いられる `Mask` と `NSP` から BERT がどのような学習をしているのかを理解します。また、文脈を考慮した単語ベクトル (Word Embedding) を出力します。\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 必要なライブラリのインポート"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "HuggingFace の Transformers を使っていきます。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import BertModel,BertJapaneseTokenizer, BertForMaskedLM, BertForNextSentencePrediction"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## モデルの準備\n",
    "学習済みモデルの tokenizer と model を取得します。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# tokenizer と model の取得\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "model = BertForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "model.eval()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mask"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "一部の単語を MASK し MASK された箇所に入る単語を予測するタスク"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# オリジナルの文章\n",
    "text = 'クラウド上で機械学習を行ってモデルを作成する。'\n",
    "tokenized = tokenizer.tokenize(text)\n",
    "print(tokenized)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# MASK\n",
    "masking_index = 11\n",
    "tokenized[masking_index] = '[MASK]'\n",
    "print(tokenized)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "input_tensor = torch.tensor([ids])\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    predictions = output[0][0, masking_index].topk(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for k, i in enumerate(predictions.indices):\n",
    "    index = i.item()\n",
    "    word = tokenizer.convert_ids_to_tokens([index])[0]\n",
    "    print(k, word)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NSP"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2 つの文章が連続した文章かどうかを当てるタスク"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = BertForNextSentencePrediction.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "model.eval()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prompt = \"東京都で4月15日、新たに729人が新型コロナウイルスに感染していることがわかりました。\"\n",
    "next_sentence = \"マイクロソフトは、Azure Active Directory Basic および Premium サービスについて、99.99% の可用性を保証します。\"\n",
    "#next_sentence = \"東京都は今後もPCR検査拡充を進めていくとともに、感染経路の洗い出しを行う方針です。\"\n",
    "encoding = tokenizer(prompt, next_sentence, return_tensors='pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**encoding, labels=torch.LongTensor([1]))\n",
    "    logits = outputs.logits# %%print(outputs.logits)\n",
    "    result = F.softmax(logits, dim=-1)\n",
    "\n",
    "result = result.to('cpu').detach().numpy().copy()[0]\n",
    "print(\"True: \"+str(result[0]))\n",
    "print(\"False: \"+str(result[1]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Vector"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "文章から単語ベクトル Word Embedding を出力します。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "model = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "model.eval()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text = 'クラウド上で機械学習を行ってモデルを作成する。'\n",
    "ids = tokenizer(text,padding=True,truncation=True,return_tensors=\"pt\")\n",
    "print(ids)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**ids).last_hidden_state # 最終隱れ層\n",
    "    vec = outputs[:,0,:]\n",
    "    vec = vec.view(-1, 768)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(vec.shape)\n",
    "print(vec)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_env",
   "language": "python",
   "name": "bert_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "metadata": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}